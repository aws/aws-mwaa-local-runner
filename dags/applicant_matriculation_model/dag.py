# Dependency imports
from datetime import datetime
from airflow import DAG
import pendulum
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.redshift_to_s3 import RedshiftToS3Operator

# Dag imports
from applicant_matriculation_model.src import utils
from applicant_matriculation_model.src.get_predictions import get_predictions
from applicant_matriculation_model.src.push_predictions_to_sfdc import push_predictions_to_sfdc

# Irondata imports
from shared.irondata import Irondata
from shared.irontable import Irontable
from shared.operators.python_s3 import S3PythonOperator 
from shared.operators.s3_to_redshift_staging_operator import S3ToRedshiftStagingTransfer


# Model deployment is set here
# If set to None, the maximum version is used
MODEL_VERSION = utils.get_version(version=None)

# Setting UI Colors
PostgresOperator.ui_color = '#a597b2'

# Dag Configs
DAG_SETTINGS = dict(
    dag_id="applicant_matriculation_prediction",
    schedule_interval="0 3,15 * * *",  # Every day 3am and 3pm
    start_date=pendulum.datetime(2023, 4, 4, tz='America/New_York'),
    catchup=False,
    max_active_runs=1,
    template_searchpath=[Irondata.sql_templates_path(),
                         utils.get_sql_path(version=MODEL_VERSION, dir=True),  # Find model version sql file
                         utils.SRC,
                         (utils.ASSETS/str(MODEL_VERSION)).as_posix(),
                         ]
    )

# Collects all applications in the 12 hours preceding the execution_date
BEGIN_DATE = '{{ (execution_date - macros.timedelta(hours=12)).strftime("%Y-%m-%d %H:%M:%S") }}'
END_DATE = '{{ execution_date.strftime("%Y-%m-%d %H:%M:%S") }}'
sql_params = {
    'start_time': BEGIN_DATE,
    'end_time': END_DATE,
}

# Redshift tables generated by this dag
SCHEMA = 'inference'
predictions = Irontable(schema=SCHEMA, table='app_scoring_predictions', primary_key='id')
logs = Irontable(schema=SCHEMA, table='preprocessing_logs', primary_key='id')
# Adding a new attribute to make it easy to access the table sql path
predictions.sql_create_file = 'app_scoring_predictions.sql'
logs.sql_create_file = 'preprocessing_logs.sql'

# Common operator setttings
BUCKET = Irondata.s3_warehouse_bucket()
shared_python_s3_operator_settings = dict(
    bucket_name=BUCKET,
    aws_conn_id='aws',
    )

with DAG(**DAG_SETTINGS) as dag:

    # S3 Paths for the different datasets generated throughout the etl
    dag_id = dag._dag_id
    APPLICATIONS_S3_KEY = dag_id + '/{{ execution_date.strftime("%Y-%m-%d-%H") }}/applications.csv'
    FINAL_PREDICTORS_S3_KEY = dag_id + '/{{ execution_date.strftime("%Y-%m-%d-%H") }}/final_predictors.csv'
    APPLICATIONS_VALIDATED_S3_KEY = dag_id + (('/{{ execution_date.strftime("%Y-%m-%d-%H") }}'
                                               '/validated_applications.csv')
                                              if utils.is_required(MODEL_VERSION, 'validation')
                                              else APPLICATIONS_S3_KEY)
    LOGS_S3_KEY = dag_id + '/{{ execution_date.strftime("%Y-%m-%d-%H") }}/validation_logs.csv'
    PREDICTIONS_S3_KEY = dag_id + '/{{ execution_date.strftime("%Y-%m-%d-%H") }}/predictions.csv'

    # Mapping the tables with their final S3 staging path
    TABLES = {
        predictions: PREDICTIONS_S3_KEY,
        logs: LOGS_S3_KEY
    }

    # Collects new applications and related predictors.
    # Pushes new applications to S3.
    collect_applications = RedshiftToS3Operator(
        task_id='collect_applications',
        s3_bucket=BUCKET,
        s3_key=APPLICATIONS_S3_KEY,
        select_query=utils.load_sql(version=MODEL_VERSION, **sql_params),
        include_header=True,
        unload_options=['CSV', 'ALLOWOVERWRITE', 'PARALLEL OFF']
    )

    if utils.is_required(MODEL_VERSION, 'feature_engineering'):
        engineer_features = S3PythonOperator(
            task_id='engineer_features',
            source_s3_key=APPLICATIONS_S3_KEY + '000',
            dest_s3_key=[FINAL_PREDICTORS_S3_KEY],
            python_callable=utils.load_asset_module(MODEL_VERSION, 'feature_engineering.py').main,
            **shared_python_s3_operator_settings
        )
    else:
        # If feature_engineering is not required for the model
        # We use a dummy operator to maintain the operator sequencing
        engineer_features = DummyOperator(
            task_id='engineer_features'
        )

    if utils.is_required(MODEL_VERSION, 'validation'):
        # Pulls applications from S3.
        # Validates the data for each application
        # Pushes validated applications to S3
        # Pushes the validation logs to S3
        validate_features = S3PythonOperator(
            task_id='validate_features',
            source_s3_key=FINAL_PREDICTORS_S3_KEY,
            dest_s3_key=[APPLICATIONS_VALIDATED_S3_KEY, LOGS_S3_KEY],
            python_callable=utils.load_asset_module(MODEL_VERSION, 'validation.py').main,
            op_kwargs={'task_name': 'applicant_matriculation_model'},
            **shared_python_s3_operator_settings
            )
    else:
        # If validation is not required for the model
        # We use a dummy operator to maintain the operator sequencing
        validate_features = DummyOperator(
            task_id='validate_features'
        )

    # Pulls validated applications from S3.
    # Loads model
    # Generates predictions
    # Pushes predictions to S3
    generate_predictions = S3PythonOperator(
        task_id="generate_predictions",
        source_s3_key=APPLICATIONS_VALIDATED_S3_KEY,
        dest_s3_key=[PREDICTIONS_S3_KEY],
        python_callable=get_predictions,
        op_kwargs={
            'version': MODEL_VERSION,
        },
        **shared_python_s3_operator_settings,
    )

    # Send the predictions to Salesforce
    predictions_to_sfdc = PythonOperator(
        task_id = "predictions_to_sfdc",
        python_callable=push_predictions_to_sfdc,
        op_kwargs={
            's3_bucket': BUCKET,
            's3_key': PREDICTIONS_S3_KEY
        }
    )

    # The final transfer from s3 to staging-redshift
    # and the transfer from staging-redshift
    # to production redshift are the exact same for each production table
    # other than altered table names, and altered s3 paths.
    # Below the create, load, and stage operators are generated in a for loop.
    staging_ops = []
    create_staging_ops = []
    create_table_ops = []
    for table, s3_key in TABLES.items():

        create_table_op = PostgresOperator(
            task_id=f"create_table__{table.table}",
            sql=table.sql_create_file,
            params={
                "schema": table.schema_in_env,
                "table": table.table_in_env,
            },
            autocommit=True,
            # trigger_rule=TriggerRule.DUMMY
        )

        create_staging_table_op = PostgresOperator(
            dag=dag,
            task_id=f"create_{table.table}_staging_table",
            sql="create_staging_table_autoincrement.sql",
            params={
                "schema": table.schema_in_env,
                "table": table.table_in_env,
                "pk": table.primary_key,
                "strftime": "%Y-%m-%d-%H",
            },
            autocommit=True)

        staging_op = S3ToRedshiftStagingTransfer(
            task_id=f"S3_to_redshift__{table.table}",
            s3_key=dag.dag_id,
            schema=table.schema_in_env,
            table=table.table_in_env,
            strftime="%Y-%m-%d-%H",
            parameters=dict(
                entity_name=s3_key.split('/')[-1],
            ),
            s3_bucket=BUCKET,
            copy_options=["csv", "IGNOREHEADER 1"],
        )

        load_op = PostgresOperator(
            task_id=f"staging_to_prod_{table.table}",
            sql="load_staged_data_autoincrement.sql",
            params={
                "schema": table.schema_in_env,
                "table": table.table_in_env,
                "pk": table.primary_key,
                "strftime": "%Y-%m-%d-%H"
            },
            autocommit=True
        )

        create_staging_ops.append(create_staging_table_op)
        create_table_ops.append(create_table_op)
        staging_ops.append(staging_op)

        create_table_op >> create_staging_table_op >> staging_op >> load_op

    collect_applications >> engineer_features
    engineer_features >> validate_features
    validate_features >> generate_predictions
    generate_predictions >> staging_ops
    generate_predictions >> predictions_to_sfdc
