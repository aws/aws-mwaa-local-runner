# irondata imports
import string
from shared.irondata import Irondata
from shared.irontable import Irontable
from shared.dag_factory import create_dag
from shared.operators.python_s3 import S3PythonOperator
from shared.operators.s3_to_redshift_staging_operator import S3ToRedshiftStagingTransfer
from canvas_page_views.src import get_page_views, collect_ids

# base python imports
from pathlib import Path
from datetime import datetime
from types import FunctionType

# airflow imports
from airflow.operators.postgres_operator import PostgresOperator

# dag config
REDSHIFT_CONN_ID = "redshift"
S3_CONN_ID = "aws"
BUCKET = Irondata.s3_warehouse_bucket()
PARALLEL_SPLITS = 10
TABLE = Irontable(
    schema="fis", 
    table="canvas_page_views", 
    primary_key="id",
    )
DAG_CONFIG = dict(
    dag_name="canvas_page_views",
    schedule="0 */12 * * *",
    start_date=datetime(2023, 4, 4),
    catchup=False,
    max_active_runs=1,
    tags=["API"]
    )

# load sql query for collecting active student canvas user ids
sql_path = Path(__file__).resolve().parent / 'sql' / 'student_ids.sql'
with open(sql_path, 'r') as sql_file:
    student_ids_sql = sql_file.read()

def generate_parallel_operators(
    sql: str,
    base_dataset_s3_path: str,
    prod_dataset_s3_path: str,
    base_dataset_callable: FunctionType,
    parallel_callable: FunctionType,
    splits: int,
    s3_conn_id: str,
    bucket_name: str,
    irontable: Irontable,
    redshift_conn_id: str,
    base_dataset_id: str="collect_ids",
    parallel_id: str="collect_data") -> dict:
    """
    The following steps are completed by this function:
    1. A dataset is generated by running a redshift sql query within the `base_dataset_callable`
    2. The dataset is broken into X splits
    3. Each split is pushed to S3
    4. For each split, an S3PythonOperator is generated that applies the `parallel_callable` to the split
    5. The results of each S3PythonOperator are pushed to S3
    6. An S3ToRedshiftStagingTransfer is generated for each parallel result
    7. The task dependencies are set so the parellel_callables are run in parallel
    8. Return all operators within a dictionary with the following keys
        - base_dataset_op
        - parallel_ops
        - staging_ops
    """
    # Generate the s3 paths for each split
    base_dataset_paths = [base_dataset_s3_path + f'_{i}.csv' for i in range(1, splits+1)]

    # Generate base dataset splits and push to S3
    base_dataset_op = S3PythonOperator(
        task_id=base_dataset_id,
        aws_conn_id=s3_conn_id,
        bucket_name=bucket_name,
        python_callable=base_dataset_callable,
        op_kwargs=dict(
            sql=sql, 
            splits=splits,
            ),
        dest_s3_key=base_dataset_paths,
    )

    parallel_ops = []
    staging_ops = []
    for idx, path in enumerate(base_dataset_paths, start=1):
        # Generate finalized dataset s3 path
        prod_path = prod_dataset_s3_path + f'_{idx}.csv' 
        # Tokens are made for even and odd jobs
        token_letter = string.ascii_uppercase[idx - 1]
        token = Irondata.get_config(f'CANVAS_API_TOKEN_{token_letter}')
        # Pull down base dataset split
        # Generate finalized dataset
        # Push to S3
        parallel_op = S3PythonOperator(
            task_id=parallel_id + f'_{idx}',
            retries=3,
            aws_conn_id=s3_conn_id,
            bucket_name=bucket_name,
            python_callable=parallel_callable,
            source_s3_key=path,
            dest_s3_key=prod_path,
            op_kwargs=dict(
                token=token
            )
        )

        staging_op = S3ToRedshiftStagingTransfer(
            task_id=f"s3_to_staging_{idx}",
            retries=3,
            s3_key=prod_path.split('/')[0],
            schema=irontable.schema_in_env,
            table=irontable.table_in_env,
            redshift_conn_id=redshift_conn_id,
            parameters=dict(
                entity_name=prod_path.split('/')[-1],
                ),
            s3_bucket=bucket_name,
            copy_options=[
                "csv",
                "IGNOREHEADER 1",
                "timeformat 'YYYY-MM-DD HH:MI:SS'",
                ],
        )

        parallel_op >> staging_op
        parallel_ops.append(parallel_op)
        staging_ops.append(staging_op)

    base_dataset_op >> parallel_ops

    return {
        'base_dataset_op': base_dataset_op,
        'parallel_ops': parallel_ops,
        'staging_ops': staging_ops
        }

# DAG GENERATION
with create_dag(**DAG_CONFIG) as dag:

    S3_DATASETS = ['ids', 'page_views']
    S3_KEYS = {
        dataset: '/'.join([DAG_CONFIG['dag_name'], '{{ ds }}', f'{dataset}'])
        for dataset in S3_DATASETS
        }

    prod_table = PostgresOperator(
        task_id="create_production_table",
        postgres_conn_id=REDSHIFT_CONN_ID,
        autocommit=True,
        sql="canvas_page_views.sql",
        params=TABLE.to_dict(),
        )

    staging_table = PostgresOperator(
        task_id="create_staging_table",
        sql="create_staging_tables.sql",
        postgres_conn_id=REDSHIFT_CONN_ID,
        autocommit=True,
        params=dict(
            tables=[TABLE.table_in_env],
            **TABLE.to_dict()
            ),
        )

    parallel_ops = generate_parallel_operators(
        sql=student_ids_sql,
        base_dataset_s3_path=S3_KEYS['ids'],
        prod_dataset_s3_path=S3_KEYS['page_views'],
        splits=PARALLEL_SPLITS,
        s3_conn_id=S3_CONN_ID,
        bucket_name=BUCKET,
        irontable=TABLE,
        redshift_conn_id=REDSHIFT_CONN_ID,
        base_dataset_callable=collect_ids,
        parallel_callable=get_page_views,
    )

    load_prod = PostgresOperator(
        task_id="staging_to_prod",
        sql="load_staged_data.sql",
        postgres_conn_id=REDSHIFT_CONN_ID,
        autocommit=True,
        params=dict(
            schema=TABLE.schema_in_env,
            table=TABLE.table_in_env
            ),
        )

    prod_table >> staging_table
    staging_table >> parallel_ops['staging_ops'] >> load_prod
